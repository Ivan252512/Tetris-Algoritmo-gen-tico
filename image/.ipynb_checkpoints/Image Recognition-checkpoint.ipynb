{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "\n",
    "\n",
    "config = tf.ConfigProto( device_count = {'GPU': 1 , 'CPU': 10} ) \n",
    "sess = tf.Session(config=config) \n",
    "keras.backend.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential,Input,Model\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.advanced_activations import LeakyReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "leyendo imagenes de  C:\\Users\\UsX\\Documents\\GitHub\\Tetris-Neural-Network\\image\\trainprueba\\\n",
      "Leyendo...1\r",
      "C:\\Users\\UsX\\Documents\\GitHub\\Tetris-Neural-Network\\image\\trainprueba\\I 1\n",
      "Leyendo...1\r",
      "Leyendo...2\r",
      "Leyendo...3\r",
      "Leyendo...4\r",
      "Leyendo...5\r",
      "Leyendo...6\r",
      "Leyendo...7\r",
      "Leyendo...8\r",
      "Leyendo...9\r",
      "Leyendo...10\r",
      "Leyendo...11\r",
      "Leyendo...12\r",
      "Leyendo...13\r",
      "Leyendo...14\r",
      "Leyendo...15\r",
      "Leyendo...16\r",
      "Leyendo...17\r",
      "Leyendo...18\r",
      "Leyendo...19\r",
      "Leyendo...20\r",
      "C:\\Users\\UsX\\Documents\\GitHub\\Tetris-Neural-Network\\image\\trainprueba\\J 20\n",
      "Leyendo...1\r",
      "Leyendo...2\r",
      "Leyendo...3\r",
      "Leyendo...4\r",
      "Leyendo...5\r",
      "Leyendo...6\r",
      "Leyendo...7\r",
      "Leyendo...8\r",
      "Leyendo...9\r",
      "Leyendo...10\r",
      "Leyendo...11\r",
      "Leyendo...12\r",
      "Leyendo...13\r",
      "Leyendo...14\r",
      "Leyendo...15\r",
      "Leyendo...16\r",
      "C:\\Users\\UsX\\Documents\\GitHub\\Tetris-Neural-Network\\image\\trainprueba\\L 16\n",
      "Leyendo...1\r",
      "Leyendo...2\r",
      "Leyendo...3\r",
      "Leyendo...4\r",
      "Leyendo...5\r",
      "Leyendo...6\r",
      "Leyendo...7\r",
      "Leyendo...8\r",
      "Leyendo...9\r",
      "Leyendo...10\r",
      "Leyendo...11\r",
      "Leyendo...12\r",
      "Leyendo...13\r",
      "Leyendo...14\r",
      "Leyendo...15\r",
      "Leyendo...16\r",
      "Leyendo...17\r",
      "C:\\Users\\UsX\\Documents\\GitHub\\Tetris-Neural-Network\\image\\trainprueba\\O 17\n",
      "Leyendo...1\r",
      "Leyendo...2\r",
      "Leyendo...3\r",
      "Leyendo...4\r",
      "Leyendo...5\r",
      "Leyendo...6\r",
      "Leyendo...7\r",
      "Leyendo...8\r",
      "Leyendo...9\r",
      "Leyendo...10\r",
      "Leyendo...11\r",
      "Leyendo...12\r",
      "Leyendo...13\r",
      "Leyendo...14\r",
      "Leyendo...15\r",
      "Leyendo...16\r",
      "Leyendo...17\r",
      "Leyendo...18\r",
      "C:\\Users\\UsX\\Documents\\GitHub\\Tetris-Neural-Network\\image\\trainprueba\\S 18\n",
      "Leyendo...1\r",
      "Leyendo...2\r",
      "Leyendo...3\r",
      "Leyendo...4\r",
      "Leyendo...5\r",
      "Leyendo...6\r",
      "Leyendo...7\r",
      "Leyendo...8\r",
      "Leyendo...9\r",
      "Leyendo...10\r",
      "Leyendo...11\r",
      "Leyendo...12\r",
      "Leyendo...13\r",
      "Leyendo...14\r",
      "Leyendo...15\r",
      "Leyendo...16\r",
      "Leyendo...17\r",
      "Leyendo...18\r",
      "Leyendo...19\r",
      "Leyendo...20\r",
      "C:\\Users\\UsX\\Documents\\GitHub\\Tetris-Neural-Network\\image\\trainprueba\\T 20\n",
      "Leyendo...1\r",
      "Leyendo...2\r",
      "Leyendo...3\r",
      "Leyendo...4\r",
      "Leyendo...5\r",
      "Leyendo...6\r",
      "Leyendo...7\r",
      "Leyendo...8\r",
      "Leyendo...9\r",
      "Leyendo...10\r",
      "Leyendo...11\r",
      "Leyendo...12\r",
      "Leyendo...13\r",
      "Leyendo...14\r",
      "Leyendo...15\r",
      "Leyendo...16\r",
      "C:\\Users\\UsX\\Documents\\GitHub\\Tetris-Neural-Network\\image\\trainprueba\\Z 16\n",
      "Leyendo...1\r",
      "Leyendo...2\r",
      "Leyendo...3\r",
      "Leyendo...4\r",
      "Leyendo...5\r",
      "Leyendo...6\r",
      "Leyendo...7\r",
      "Leyendo...8\r",
      "Leyendo...9\r",
      "Leyendo...10\r",
      "Leyendo...11\r",
      "Leyendo...12\r",
      "Leyendo...13\r",
      "Leyendo...14\r",
      "Leyendo...15\r",
      "Leyendo...16\r",
      "Leyendo...17\r",
      "Directorios leidos: 7\n",
      "Imagenes en cada directorio [21, 16, 17, 18, 20, 16, 17]\n",
      "suma Total de imagenes en subdirs: 125\n"
     ]
    }
   ],
   "source": [
    "dirname = os.path.join(os.getcwd(), 'trainprueba')\n",
    "imgpath = dirname + os.sep \n",
    " \n",
    "images = []\n",
    "directories = []\n",
    "dircount = []\n",
    "prevRoot=''\n",
    "cant=0\n",
    " \n",
    "print(\"leyendo imagenes de \",imgpath)\n",
    " \n",
    "for root, dirnames, filenames in os.walk(imgpath):\n",
    "    for filename in filenames:\n",
    "        if re.search(\"\\.(jpg|jpeg|png|bmp|tiff)$\", filename):\n",
    "            cant=cant+1\n",
    "            filepath = os.path.join(root, filename)\n",
    "            image = plt.imread(filepath)\n",
    "            images.append(image)\n",
    "            b = \"Leyendo...\" + str(cant)\n",
    "            print (b, end=\"\\r\")\n",
    "            if prevRoot !=root:\n",
    "                print(root, cant)\n",
    "                prevRoot=root\n",
    "                directories.append(root)\n",
    "                dircount.append(cant)\n",
    "                cant=0\n",
    "dircount.append(cant)\n",
    " \n",
    "dircount = dircount[1:]\n",
    "dircount[0]=dircount[0]+1\n",
    "print('Directorios leidos:',len(directories))\n",
    "print(\"Imagenes en cada directorio\", dircount)\n",
    "print('suma Total de imagenes en subdirs:',sum(dircount))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad etiquetas creadas:  125\n",
      "0 I\n",
      "1 J\n",
      "2 L\n",
      "3 O\n",
      "4 S\n",
      "5 T\n",
      "6 Z\n",
      "Total number of outputs :  7\n",
      "Output classes :  [0 1 2 3 4 5 6]\n"
     ]
    }
   ],
   "source": [
    "labels=[]\n",
    "indice=0\n",
    "for cantidad in dircount:\n",
    "    for i in range(cantidad):\n",
    "        labels.append(indice)\n",
    "    indice=indice+1\n",
    "print(\"Cantidad etiquetas creadas: \",len(labels))\n",
    "\n",
    "blocks=[]\n",
    "indice=0\n",
    "for directorio in directories:\n",
    "    name = directorio.split(os.sep)\n",
    "    print(indice , name[len(name)-1])\n",
    "    blocks.append(name[len(name)-1])\n",
    "    indice=indice+1\n",
    "\n",
    "y = np.array(labels)\n",
    "X = np.array(images, dtype=np.uint8) #convierto de lista a numpy\n",
    "\n",
    "# Find the unique numbers from the train labels\n",
    "classes = np.unique(y)\n",
    "nClasses = len(classes)\n",
    "print('Total number of outputs : ', nClasses)\n",
    "print('Output classes : ', classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape :  (100, 100, 100, 4) (100,)\n",
      "Testing data shape :  (25, 100, 100, 4) (25,)\n",
      "Original label: 0\n",
      "After conversion to one-hot: [1. 0. 0. 0. 0. 0. 0.]\n",
      "(80, 100, 100, 4) (20, 100, 100, 4) (80, 7) (20, 7)\n"
     ]
    }
   ],
   "source": [
    "#Mezclar todo y crear los grupos de entrenamiento y testing\n",
    "train_X,test_X,train_Y,test_Y = train_test_split(X,y,test_size=0.2)\n",
    "print('Training data shape : ', train_X.shape, train_Y.shape)\n",
    "print('Testing data shape : ', test_X.shape, test_Y.shape)\n",
    "\n",
    "train_X = train_X.astype('float32')\n",
    "test_X = test_X.astype('float32')\n",
    "train_X = train_X / 255.\n",
    "test_X = test_X / 255.\n",
    "\n",
    "# Change the labels from categorical to one-hot encoding\n",
    "train_Y_one_hot = to_categorical(train_Y)\n",
    "test_Y_one_hot = to_categorical(test_Y)\n",
    "\n",
    "# Display the change for category label using one-hot encoding\n",
    "print('Original label:', train_Y[0])\n",
    "print('After conversion to one-hot:', train_Y_one_hot[0])\n",
    "\n",
    "train_X,valid_X,train_label,valid_label = train_test_split(train_X, train_Y_one_hot, test_size=0.2, random_state=13)\n",
    "\n",
    "print(train_X.shape,valid_X.shape,train_label.shape,valid_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_4 (Conv2D)            (None, 100, 100, 128)     18560     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)    (None, 100, 100, 128)     0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 25, 25, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 25, 25, 128)       0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 80000)             0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 128)               10240128  \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 7)                 903       \n",
      "=================================================================\n",
      "Total params: 10,259,591\n",
      "Trainable params: 10,259,591\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "INIT_LR = 1e-3\n",
    "epochs = 256\n",
    "batch_size = 256\n",
    "\n",
    "block_model = Sequential()\n",
    "block_model.add(Conv2D(128, kernel_size=(6, 6),activation='linear',padding='same',input_shape=(100,100,3)))\n",
    "block_model.add(LeakyReLU(alpha=0.1))\n",
    "block_model.add(MaxPooling2D((4, 4),padding='same'))\n",
    "block_model.add(Dropout(0.5))\n",
    "\n",
    "block_model.add(Flatten())\n",
    "block_model.add(Dense(128, activation='linear'))\n",
    "block_model.add(LeakyReLU(alpha=0.1))\n",
    "block_model.add(Dropout(0.5)) \n",
    "block_model.add(Dense(nClasses, activation='softmax'))\n",
    "\n",
    "block_model.summary()\n",
    "\n",
    "block_model.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.Adagrad(lr=INIT_LR, decay=INIT_LR / 100),metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 80 samples, validate on 20 samples\n",
      "Epoch 1/256\n",
      "80/80 [==============================] - 1s 6ms/step - loss: 1.9459 - acc: 0.1000 - val_loss: 1.9523 - val_acc: 0.1000\n",
      "Epoch 2/256\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 1.9424 - acc: 0.1750 - val_loss: 1.9584 - val_acc: 0.2000\n",
      "Epoch 3/256\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 1.9293 - acc: 0.2500 - val_loss: 1.9927 - val_acc: 0.3000\n",
      "Epoch 4/256\n",
      "80/80 [==============================] - 0s 975us/step - loss: 1.9298 - acc: 0.1625 - val_loss: 1.9714 - val_acc: 0.1000\n",
      "Epoch 5/256\n",
      "80/80 [==============================] - 0s 938us/step - loss: 1.9088 - acc: 0.2250 - val_loss: 2.0024 - val_acc: 0.1000\n",
      "Epoch 6/256\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 1.9147 - acc: 0.2500 - val_loss: 1.9886 - val_acc: 0.1000\n",
      "Epoch 7/256\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 1.9178 - acc: 0.2000 - val_loss: 1.9537 - val_acc: 0.2000\n",
      "Epoch 8/256\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 1.9022 - acc: 0.2500 - val_loss: 1.9845 - val_acc: 0.2000\n",
      "Epoch 9/256\n",
      "80/80 [==============================] - 0s 963us/step - loss: 1.9135 - acc: 0.2375 - val_loss: 1.9564 - val_acc: 0.2000\n",
      "Epoch 10/256\n",
      "80/80 [==============================] - 0s 950us/step - loss: 1.9070 - acc: 0.2000 - val_loss: 1.9524 - val_acc: 0.2000\n",
      "Epoch 11/256\n",
      "80/80 [==============================] - 0s 950us/step - loss: 1.8762 - acc: 0.3250 - val_loss: 1.9797 - val_acc: 0.2000\n",
      "Epoch 12/256\n",
      "80/80 [==============================] - 0s 938us/step - loss: 1.8967 - acc: 0.2250 - val_loss: 1.9515 - val_acc: 0.2000\n",
      "Epoch 13/256\n",
      "80/80 [==============================] - 0s 963us/step - loss: 1.8849 - acc: 0.3250 - val_loss: 1.9519 - val_acc: 0.2000\n",
      "Epoch 14/256\n",
      "80/80 [==============================] - 0s 938us/step - loss: 1.8799 - acc: 0.2750 - val_loss: 1.9523 - val_acc: 0.2000\n",
      "Epoch 15/256\n",
      "80/80 [==============================] - 0s 938us/step - loss: 1.8802 - acc: 0.3625 - val_loss: 1.9400 - val_acc: 0.3000\n",
      "Epoch 16/256\n",
      "80/80 [==============================] - 0s 938us/step - loss: 1.8749 - acc: 0.2500 - val_loss: 1.9363 - val_acc: 0.2000\n",
      "Epoch 17/256\n",
      "80/80 [==============================] - 0s 950us/step - loss: 1.8590 - acc: 0.3625 - val_loss: 1.9269 - val_acc: 0.3000\n",
      "Epoch 18/256\n",
      "80/80 [==============================] - 0s 975us/step - loss: 1.8669 - acc: 0.3375 - val_loss: 1.9113 - val_acc: 0.3000\n",
      "Epoch 19/256\n",
      "80/80 [==============================] - 0s 963us/step - loss: 1.8396 - acc: 0.4250 - val_loss: 1.9014 - val_acc: 0.3000\n",
      "Epoch 20/256\n",
      "80/80 [==============================] - 0s 963us/step - loss: 1.8466 - acc: 0.3625 - val_loss: 1.8976 - val_acc: 0.3000\n",
      "Epoch 21/256\n",
      "80/80 [==============================] - 0s 950us/step - loss: 1.8444 - acc: 0.4625 - val_loss: 1.9029 - val_acc: 0.3000\n",
      "Epoch 22/256\n",
      "80/80 [==============================] - 0s 963us/step - loss: 1.8578 - acc: 0.4000 - val_loss: 1.8764 - val_acc: 0.3000\n",
      "Epoch 23/256\n",
      "80/80 [==============================] - 0s 988us/step - loss: 1.8398 - acc: 0.5000 - val_loss: 1.8753 - val_acc: 0.3000\n",
      "Epoch 24/256\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 1.8270 - acc: 0.4750 - val_loss: 1.8786 - val_acc: 0.3000\n",
      "Epoch 25/256\n",
      "80/80 [==============================] - 0s 988us/step - loss: 1.8110 - acc: 0.4125 - val_loss: 1.8776 - val_acc: 0.3000\n",
      "Epoch 26/256\n",
      "80/80 [==============================] - 0s 950us/step - loss: 1.8138 - acc: 0.3750 - val_loss: 1.8698 - val_acc: 0.3000\n",
      "Epoch 27/256\n",
      "80/80 [==============================] - 0s 963us/step - loss: 1.7835 - acc: 0.4625 - val_loss: 1.8796 - val_acc: 0.3000\n",
      "Epoch 28/256\n",
      "80/80 [==============================] - 0s 950us/step - loss: 1.7805 - acc: 0.4875 - val_loss: 1.8680 - val_acc: 0.3000\n",
      "Epoch 29/256\n",
      "80/80 [==============================] - 0s 938us/step - loss: 1.7713 - acc: 0.5125 - val_loss: 1.8539 - val_acc: 0.3000\n",
      "Epoch 30/256\n",
      "80/80 [==============================] - 0s 963us/step - loss: 1.7700 - acc: 0.4500 - val_loss: 1.8469 - val_acc: 0.3000\n",
      "Epoch 31/256\n",
      "80/80 [==============================] - 0s 938us/step - loss: 1.7722 - acc: 0.4375 - val_loss: 1.8414 - val_acc: 0.3000\n",
      "Epoch 32/256\n",
      "80/80 [==============================] - 0s 938us/step - loss: 1.7664 - acc: 0.4875 - val_loss: 1.8357 - val_acc: 0.3000\n",
      "Epoch 33/256\n",
      "80/80 [==============================] - 0s 963us/step - loss: 1.7569 - acc: 0.4375 - val_loss: 1.8291 - val_acc: 0.3000\n",
      "Epoch 34/256\n",
      "80/80 [==============================] - 0s 963us/step - loss: 1.7371 - acc: 0.4875 - val_loss: 1.8280 - val_acc: 0.3000\n",
      "Epoch 35/256\n",
      "80/80 [==============================] - 0s 963us/step - loss: 1.7205 - acc: 0.5000 - val_loss: 1.8268 - val_acc: 0.3000\n",
      "Epoch 36/256\n",
      "80/80 [==============================] - 0s 951us/step - loss: 1.7472 - acc: 0.4750 - val_loss: 1.8096 - val_acc: 0.3000\n",
      "Epoch 37/256\n",
      "80/80 [==============================] - 0s 950us/step - loss: 1.7173 - acc: 0.5000 - val_loss: 1.8065 - val_acc: 0.3000\n",
      "Epoch 38/256\n",
      "80/80 [==============================] - 0s 950us/step - loss: 1.7022 - acc: 0.5125 - val_loss: 1.8029 - val_acc: 0.3000\n",
      "Epoch 39/256\n",
      "80/80 [==============================] - 0s 950us/step - loss: 1.7357 - acc: 0.4750 - val_loss: 1.7784 - val_acc: 0.3000\n",
      "Epoch 40/256\n",
      "80/80 [==============================] - 0s 950us/step - loss: 1.7079 - acc: 0.5250 - val_loss: 1.7749 - val_acc: 0.3000\n",
      "Epoch 41/256\n",
      "80/80 [==============================] - 0s 963us/step - loss: 1.6621 - acc: 0.5750 - val_loss: 1.7869 - val_acc: 0.3000\n",
      "Epoch 42/256\n",
      "80/80 [==============================] - 0s 950us/step - loss: 1.6625 - acc: 0.5250 - val_loss: 1.7778 - val_acc: 0.3000\n",
      "Epoch 43/256\n",
      "80/80 [==============================] - 0s 963us/step - loss: 1.6990 - acc: 0.4750 - val_loss: 1.7494 - val_acc: 0.3000\n",
      "Epoch 44/256\n",
      "80/80 [==============================] - 0s 963us/step - loss: 1.6713 - acc: 0.5000 - val_loss: 1.7413 - val_acc: 0.3000\n",
      "Epoch 45/256\n",
      "80/80 [==============================] - 0s 938us/step - loss: 1.6654 - acc: 0.5000 - val_loss: 1.7441 - val_acc: 0.3000\n",
      "Epoch 46/256\n",
      "80/80 [==============================] - 0s 963us/step - loss: 1.6391 - acc: 0.5250 - val_loss: 1.7383 - val_acc: 0.3000\n",
      "Epoch 47/256\n",
      "80/80 [==============================] - 0s 938us/step - loss: 1.6691 - acc: 0.4875 - val_loss: 1.7234 - val_acc: 0.3000\n",
      "Epoch 48/256\n",
      "80/80 [==============================] - 0s 963us/step - loss: 1.6376 - acc: 0.5250 - val_loss: 1.7169 - val_acc: 0.3000\n",
      "Epoch 49/256\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 1.6146 - acc: 0.5500 - val_loss: 1.7151 - val_acc: 0.3000\n",
      "Epoch 50/256\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 1.6588 - acc: 0.5375 - val_loss: 1.6978 - val_acc: 0.3000\n",
      "Epoch 51/256\n",
      "80/80 [==============================] - 0s 988us/step - loss: 1.6290 - acc: 0.6375 - val_loss: 1.6945 - val_acc: 0.4500\n",
      "Epoch 52/256\n",
      "80/80 [==============================] - 0s 963us/step - loss: 1.6153 - acc: 0.5750 - val_loss: 1.6836 - val_acc: 0.4500\n",
      "Epoch 53/256\n",
      "80/80 [==============================] - 0s 963us/step - loss: 1.6103 - acc: 0.5500 - val_loss: 1.6719 - val_acc: 0.4500\n",
      "Epoch 54/256\n",
      "80/80 [==============================] - 0s 988us/step - loss: 1.6060 - acc: 0.5625 - val_loss: 1.6631 - val_acc: 0.4500\n",
      "Epoch 55/256\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 1.6011 - acc: 0.5750 - val_loss: 1.6535 - val_acc: 0.4500\n",
      "Epoch 56/256\n",
      "80/80 [==============================] - 0s 950us/step - loss: 1.5807 - acc: 0.5875 - val_loss: 1.6437 - val_acc: 0.4500\n",
      "Epoch 57/256\n",
      "80/80 [==============================] - 0s 963us/step - loss: 1.5531 - acc: 0.6125 - val_loss: 1.6422 - val_acc: 0.4500\n",
      "Epoch 58/256\n",
      "80/80 [==============================] - 0s 963us/step - loss: 1.5621 - acc: 0.6250 - val_loss: 1.6325 - val_acc: 0.4500\n",
      "Epoch 59/256\n",
      "80/80 [==============================] - 0s 963us/step - loss: 1.5310 - acc: 0.6500 - val_loss: 1.6227 - val_acc: 0.4500\n",
      "Epoch 60/256\n",
      "80/80 [==============================] - 0s 963us/step - loss: 1.5279 - acc: 0.6250 - val_loss: 1.6192 - val_acc: 0.4500\n",
      "Epoch 61/256\n",
      "80/80 [==============================] - 0s 963us/step - loss: 1.5425 - acc: 0.6125 - val_loss: 1.6091 - val_acc: 0.4500\n",
      "Epoch 62/256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80/80 [==============================] - 0s 963us/step - loss: 1.5231 - acc: 0.6125 - val_loss: 1.6066 - val_acc: 0.4500\n",
      "Epoch 63/256\n",
      "80/80 [==============================] - 0s 963us/step - loss: 1.5287 - acc: 0.6000 - val_loss: 1.5926 - val_acc: 0.4500\n",
      "Epoch 64/256\n",
      "80/80 [==============================] - 0s 963us/step - loss: 1.4742 - acc: 0.6750 - val_loss: 1.5989 - val_acc: 0.4500\n",
      "Epoch 65/256\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 1.5031 - acc: 0.5625 - val_loss: 1.5887 - val_acc: 0.4500\n",
      "Epoch 66/256\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 1.5172 - acc: 0.6000 - val_loss: 1.5877 - val_acc: 0.4500\n",
      "Epoch 67/256\n",
      "80/80 [==============================] - 0s 963us/step - loss: 1.4715 - acc: 0.6250 - val_loss: 1.5705 - val_acc: 0.4500\n",
      "Epoch 68/256\n",
      "80/80 [==============================] - 0s 950us/step - loss: 1.4905 - acc: 0.6250 - val_loss: 1.5564 - val_acc: 0.4500\n",
      "Epoch 69/256\n",
      "80/80 [==============================] - 0s 975us/step - loss: 1.4461 - acc: 0.7375 - val_loss: 1.5459 - val_acc: 0.6500\n",
      "Epoch 70/256\n",
      "80/80 [==============================] - 0s 963us/step - loss: 1.4561 - acc: 0.6750 - val_loss: 1.5408 - val_acc: 0.4500\n",
      "Epoch 71/256\n",
      "80/80 [==============================] - 0s 950us/step - loss: 1.4907 - acc: 0.6750 - val_loss: 1.5260 - val_acc: 0.6500\n",
      "Epoch 72/256\n",
      "80/80 [==============================] - 0s 963us/step - loss: 1.4425 - acc: 0.6875 - val_loss: 1.5300 - val_acc: 0.4500\n",
      "Epoch 73/256\n",
      "80/80 [==============================] - 0s 938us/step - loss: 1.4255 - acc: 0.6500 - val_loss: 1.5206 - val_acc: 0.4500\n",
      "Epoch 74/256\n",
      "80/80 [==============================] - 0s 963us/step - loss: 1.4270 - acc: 0.6250 - val_loss: 1.5102 - val_acc: 0.4500\n",
      "Epoch 75/256\n",
      "80/80 [==============================] - 0s 963us/step - loss: 1.4089 - acc: 0.7375 - val_loss: 1.5021 - val_acc: 0.4500\n",
      "Epoch 76/256\n",
      "80/80 [==============================] - 0s 963us/step - loss: 1.3776 - acc: 0.7000 - val_loss: 1.5037 - val_acc: 0.4500\n",
      "Epoch 77/256\n",
      "80/80 [==============================] - 0s 950us/step - loss: 1.4267 - acc: 0.6375 - val_loss: 1.4928 - val_acc: 0.4500\n",
      "Epoch 78/256\n",
      "80/80 [==============================] - 0s 950us/step - loss: 1.4235 - acc: 0.6500 - val_loss: 1.4908 - val_acc: 0.4500\n",
      "Epoch 79/256\n",
      "80/80 [==============================] - 0s 950us/step - loss: 1.4325 - acc: 0.5750 - val_loss: 1.4660 - val_acc: 0.6500\n",
      "Epoch 80/256\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 1.4008 - acc: 0.7500 - val_loss: 1.4632 - val_acc: 0.6500\n",
      "Epoch 81/256\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 1.3857 - acc: 0.7375 - val_loss: 1.4618 - val_acc: 0.8000\n",
      "Epoch 82/256\n",
      "80/80 [==============================] - 0s 950us/step - loss: 1.3674 - acc: 0.7500 - val_loss: 1.4614 - val_acc: 0.6500\n",
      "Epoch 83/256\n",
      "80/80 [==============================] - 0s 975us/step - loss: 1.3889 - acc: 0.6375 - val_loss: 1.4375 - val_acc: 0.8000\n",
      "Epoch 84/256\n",
      "80/80 [==============================] - 0s 963us/step - loss: 1.3316 - acc: 0.7875 - val_loss: 1.4398 - val_acc: 0.6500\n",
      "Epoch 85/256\n",
      "80/80 [==============================] - 0s 988us/step - loss: 1.3571 - acc: 0.7250 - val_loss: 1.4299 - val_acc: 0.8000\n",
      "Epoch 86/256\n",
      "80/80 [==============================] - 0s 988us/step - loss: 1.3317 - acc: 0.7500 - val_loss: 1.4250 - val_acc: 0.7500\n",
      "Epoch 87/256\n",
      "80/80 [==============================] - 0s 975us/step - loss: 1.3674 - acc: 0.7125 - val_loss: 1.4209 - val_acc: 0.8000\n",
      "Epoch 88/256\n",
      "80/80 [==============================] - 0s 963us/step - loss: 1.3076 - acc: 0.8125 - val_loss: 1.4068 - val_acc: 0.8000\n",
      "Epoch 89/256\n",
      "80/80 [==============================] - 0s 988us/step - loss: 1.3124 - acc: 0.7125 - val_loss: 1.4083 - val_acc: 0.8000\n",
      "Epoch 90/256\n",
      "80/80 [==============================] - 0s 963us/step - loss: 1.3384 - acc: 0.6375 - val_loss: 1.3882 - val_acc: 0.9500\n",
      "Epoch 91/256\n",
      "80/80 [==============================] - 0s 963us/step - loss: 1.3061 - acc: 0.8000 - val_loss: 1.3859 - val_acc: 0.8000\n",
      "Epoch 92/256\n",
      "80/80 [==============================] - 0s 963us/step - loss: 1.3049 - acc: 0.6625 - val_loss: 1.3797 - val_acc: 0.8000\n",
      "Epoch 93/256\n",
      "80/80 [==============================] - 0s 975us/step - loss: 1.3320 - acc: 0.6875 - val_loss: 1.3771 - val_acc: 0.7500\n",
      "Epoch 94/256\n",
      "80/80 [==============================] - 0s 963us/step - loss: 1.2698 - acc: 0.8000 - val_loss: 1.3702 - val_acc: 0.7500\n",
      "Epoch 95/256\n",
      "80/80 [==============================] - 0s 988us/step - loss: 1.2671 - acc: 0.7125 - val_loss: 1.3612 - val_acc: 0.8000\n",
      "Epoch 96/256\n",
      "80/80 [==============================] - 0s 975us/step - loss: 1.2707 - acc: 0.7375 - val_loss: 1.3586 - val_acc: 0.8000\n",
      "Epoch 97/256\n",
      "80/80 [==============================] - 0s 975us/step - loss: 1.2650 - acc: 0.7375 - val_loss: 1.3483 - val_acc: 0.9500\n",
      "Epoch 98/256\n",
      "80/80 [==============================] - 0s 963us/step - loss: 1.2924 - acc: 0.7750 - val_loss: 1.3424 - val_acc: 0.9500\n",
      "Epoch 99/256\n",
      "80/80 [==============================] - 0s 988us/step - loss: 1.2737 - acc: 0.7625 - val_loss: 1.3326 - val_acc: 0.9500\n",
      "Epoch 100/256\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 1.2026 - acc: 0.8250 - val_loss: 1.3317 - val_acc: 0.9500\n",
      "Epoch 101/256\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 1.2640 - acc: 0.7250 - val_loss: 1.3294 - val_acc: 0.8000\n",
      "Epoch 102/256\n",
      "80/80 [==============================] - 0s 988us/step - loss: 1.2068 - acc: 0.8000 - val_loss: 1.3130 - val_acc: 0.8000\n",
      "Epoch 103/256\n",
      "80/80 [==============================] - 0s 963us/step - loss: 1.2113 - acc: 0.7750 - val_loss: 1.3091 - val_acc: 0.8000\n",
      "Epoch 104/256\n",
      "80/80 [==============================] - 0s 975us/step - loss: 1.2289 - acc: 0.7875 - val_loss: 1.2936 - val_acc: 0.9500\n",
      "Epoch 105/256\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 1.2105 - acc: 0.7625 - val_loss: 1.2930 - val_acc: 0.9500\n",
      "Epoch 106/256\n",
      "80/80 [==============================] - 0s 963us/step - loss: 1.1923 - acc: 0.7875 - val_loss: 1.2924 - val_acc: 0.9500\n",
      "Epoch 107/256\n",
      "80/80 [==============================] - 0s 988us/step - loss: 1.2201 - acc: 0.7500 - val_loss: 1.2870 - val_acc: 0.9500\n",
      "Epoch 108/256\n",
      "80/80 [==============================] - 0s 975us/step - loss: 1.1838 - acc: 0.8375 - val_loss: 1.2770 - val_acc: 0.9500\n",
      "Epoch 109/256\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 1.1980 - acc: 0.8000 - val_loss: 1.2677 - val_acc: 0.9500\n",
      "Epoch 110/256\n",
      "80/80 [==============================] - 0s 950us/step - loss: 1.2107 - acc: 0.8125 - val_loss: 1.2651 - val_acc: 0.9500\n",
      "Epoch 111/256\n",
      "80/80 [==============================] - 0s 963us/step - loss: 1.1838 - acc: 0.7750 - val_loss: 1.2492 - val_acc: 0.9500\n",
      "Epoch 112/256\n",
      "80/80 [==============================] - 0s 988us/step - loss: 1.1409 - acc: 0.8375 - val_loss: 1.2467 - val_acc: 0.9500\n",
      "Epoch 113/256\n",
      "80/80 [==============================] - 0s 963us/step - loss: 1.1715 - acc: 0.8375 - val_loss: 1.2409 - val_acc: 0.9500\n",
      "Epoch 114/256\n",
      "80/80 [==============================] - 0s 988us/step - loss: 1.1489 - acc: 0.8375 - val_loss: 1.2304 - val_acc: 0.9500\n",
      "Epoch 115/256\n",
      "80/80 [==============================] - 0s 988us/step - loss: 1.1715 - acc: 0.7875 - val_loss: 1.2202 - val_acc: 0.9500\n",
      "Epoch 116/256\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 1.1250 - acc: 0.8625 - val_loss: 1.2111 - val_acc: 0.9500\n",
      "Epoch 117/256\n",
      "80/80 [==============================] - 0s 988us/step - loss: 1.1655 - acc: 0.7625 - val_loss: 1.2144 - val_acc: 0.9500\n",
      "Epoch 118/256\n",
      "80/80 [==============================] - 0s 975us/step - loss: 1.1408 - acc: 0.8375 - val_loss: 1.2059 - val_acc: 0.9500\n",
      "Epoch 119/256\n",
      "80/80 [==============================] - 0s 975us/step - loss: 1.1494 - acc: 0.8375 - val_loss: 1.2068 - val_acc: 0.9500\n",
      "Epoch 120/256\n",
      "80/80 [==============================] - 0s 988us/step - loss: 1.1397 - acc: 0.8125 - val_loss: 1.1993 - val_acc: 0.9500\n",
      "Epoch 121/256\n",
      "80/80 [==============================] - 0s 988us/step - loss: 1.0965 - acc: 0.8125 - val_loss: 1.1942 - val_acc: 0.9500\n",
      "Epoch 122/256\n",
      "80/80 [==============================] - 0s 950us/step - loss: 1.0953 - acc: 0.8000 - val_loss: 1.1854 - val_acc: 0.9500\n",
      "Epoch 123/256\n",
      "80/80 [==============================] - 0s 963us/step - loss: 1.0921 - acc: 0.8000 - val_loss: 1.1760 - val_acc: 0.9500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 124/256\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 1.1202 - acc: 0.7750 - val_loss: 1.1729 - val_acc: 0.9500\n",
      "Epoch 125/256\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 1.0953 - acc: 0.8000 - val_loss: 1.1700 - val_acc: 0.9500\n",
      "Epoch 126/256\n",
      "80/80 [==============================] - 0s 963us/step - loss: 1.0845 - acc: 0.7500 - val_loss: 1.1518 - val_acc: 0.9500\n",
      "Epoch 127/256\n",
      "80/80 [==============================] - 0s 938us/step - loss: 1.0737 - acc: 0.8625 - val_loss: 1.1472 - val_acc: 0.9500\n",
      "Epoch 128/256\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 1.0486 - acc: 0.9000 - val_loss: 1.1468 - val_acc: 0.9500\n",
      "Epoch 129/256\n",
      "80/80 [==============================] - 0s 950us/step - loss: 1.0542 - acc: 0.8750 - val_loss: 1.1329 - val_acc: 0.9500\n",
      "Epoch 130/256\n",
      "80/80 [==============================] - 0s 963us/step - loss: 1.0767 - acc: 0.8250 - val_loss: 1.1380 - val_acc: 0.9500\n",
      "Epoch 131/256\n",
      "80/80 [==============================] - 0s 975us/step - loss: 1.0315 - acc: 0.8500 - val_loss: 1.1247 - val_acc: 0.9500\n",
      "Epoch 132/256\n",
      "80/80 [==============================] - 0s 963us/step - loss: 1.0575 - acc: 0.8125 - val_loss: 1.1245 - val_acc: 0.9500\n",
      "Epoch 133/256\n",
      "80/80 [==============================] - 0s 963us/step - loss: 1.0067 - acc: 0.8375 - val_loss: 1.1207 - val_acc: 0.9500\n",
      "Epoch 134/256\n",
      "80/80 [==============================] - 0s 988us/step - loss: 1.0266 - acc: 0.8500 - val_loss: 1.1207 - val_acc: 0.9500\n",
      "Epoch 135/256\n",
      "80/80 [==============================] - 0s 963us/step - loss: 0.9738 - acc: 0.9125 - val_loss: 1.1088 - val_acc: 0.9500\n",
      "Epoch 136/256\n",
      "80/80 [==============================] - 0s 975us/step - loss: 1.0491 - acc: 0.8500 - val_loss: 1.1061 - val_acc: 0.9500\n",
      "Epoch 137/256\n",
      "80/80 [==============================] - 0s 950us/step - loss: 1.0277 - acc: 0.8750 - val_loss: 1.0981 - val_acc: 0.9500\n",
      "Epoch 138/256\n",
      "80/80 [==============================] - 0s 988us/step - loss: 1.0112 - acc: 0.8250 - val_loss: 1.0960 - val_acc: 0.9500\n",
      "Epoch 139/256\n",
      "80/80 [==============================] - 0s 950us/step - loss: 1.0103 - acc: 0.8750 - val_loss: 1.0886 - val_acc: 0.9500\n",
      "Epoch 140/256\n",
      "80/80 [==============================] - 0s 938us/step - loss: 1.0044 - acc: 0.8875 - val_loss: 1.0835 - val_acc: 0.9500\n",
      "Epoch 141/256\n",
      "80/80 [==============================] - 0s 950us/step - loss: 0.9956 - acc: 0.8375 - val_loss: 1.0755 - val_acc: 0.9500\n",
      "Epoch 142/256\n",
      "80/80 [==============================] - 0s 938us/step - loss: 0.9846 - acc: 0.8625 - val_loss: 1.0710 - val_acc: 0.9500\n",
      "Epoch 143/256\n",
      "80/80 [==============================] - 0s 950us/step - loss: 1.0036 - acc: 0.8750 - val_loss: 1.0641 - val_acc: 0.9500\n",
      "Epoch 144/256\n",
      "80/80 [==============================] - 0s 938us/step - loss: 0.9901 - acc: 0.8625 - val_loss: 1.0551 - val_acc: 0.9500\n",
      "Epoch 145/256\n",
      "80/80 [==============================] - 0s 938us/step - loss: 0.9832 - acc: 0.8750 - val_loss: 1.0547 - val_acc: 0.9500\n",
      "Epoch 146/256\n",
      "80/80 [==============================] - 0s 988us/step - loss: 0.9463 - acc: 0.8625 - val_loss: 1.0422 - val_acc: 0.9500\n",
      "Epoch 147/256\n",
      "80/80 [==============================] - 0s 963us/step - loss: 0.9879 - acc: 0.8875 - val_loss: 1.0505 - val_acc: 0.9500\n",
      "Epoch 148/256\n",
      "80/80 [==============================] - 0s 988us/step - loss: 0.9913 - acc: 0.8750 - val_loss: 1.0343 - val_acc: 0.9500\n",
      "Epoch 149/256\n",
      "80/80 [==============================] - 0s 975us/step - loss: 0.9967 - acc: 0.8625 - val_loss: 1.0416 - val_acc: 0.9500\n",
      "Epoch 150/256\n",
      "80/80 [==============================] - 0s 975us/step - loss: 0.9629 - acc: 0.8875 - val_loss: 1.0298 - val_acc: 0.9500\n",
      "Epoch 151/256\n",
      "80/80 [==============================] - 0s 963us/step - loss: 0.9418 - acc: 0.8875 - val_loss: 1.0346 - val_acc: 0.9500\n",
      "Epoch 152/256\n",
      "80/80 [==============================] - 0s 950us/step - loss: 0.9929 - acc: 0.8875 - val_loss: 1.0234 - val_acc: 0.9500\n",
      "Epoch 153/256\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.9407 - acc: 0.8875 - val_loss: 1.0133 - val_acc: 0.9500\n",
      "Epoch 154/256\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.9546 - acc: 0.8625 - val_loss: 1.0046 - val_acc: 0.9500\n",
      "Epoch 155/256\n",
      "80/80 [==============================] - 0s 950us/step - loss: 0.9467 - acc: 0.8750 - val_loss: 1.0067 - val_acc: 0.9500\n",
      "Epoch 156/256\n",
      "80/80 [==============================] - 0s 963us/step - loss: 0.9233 - acc: 0.9000 - val_loss: 0.9979 - val_acc: 0.9500\n",
      "Epoch 157/256\n",
      "80/80 [==============================] - 0s 950us/step - loss: 0.9099 - acc: 0.8750 - val_loss: 0.9942 - val_acc: 0.9500\n",
      "Epoch 158/256\n",
      "80/80 [==============================] - 0s 988us/step - loss: 0.9163 - acc: 0.8875 - val_loss: 0.9928 - val_acc: 0.9500\n",
      "Epoch 159/256\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.9172 - acc: 0.8750 - val_loss: 0.9933 - val_acc: 0.9500\n",
      "Epoch 160/256\n",
      "80/80 [==============================] - 0s 988us/step - loss: 0.9320 - acc: 0.9000 - val_loss: 0.9829 - val_acc: 0.9500\n",
      "Epoch 161/256\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.8986 - acc: 0.8625 - val_loss: 0.9812 - val_acc: 0.9500\n",
      "Epoch 162/256\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.8976 - acc: 0.8875 - val_loss: 0.9805 - val_acc: 0.9500\n",
      "Epoch 163/256\n",
      "80/80 [==============================] - 0s 963us/step - loss: 0.8856 - acc: 0.8625 - val_loss: 0.9719 - val_acc: 0.9500\n",
      "Epoch 164/256\n",
      "80/80 [==============================] - 0s 963us/step - loss: 0.9209 - acc: 0.8875 - val_loss: 0.9706 - val_acc: 0.9500\n",
      "Epoch 165/256\n",
      "80/80 [==============================] - 0s 950us/step - loss: 0.9117 - acc: 0.8750 - val_loss: 0.9673 - val_acc: 0.9500\n",
      "Epoch 166/256\n",
      "80/80 [==============================] - 0s 963us/step - loss: 0.9334 - acc: 0.8625 - val_loss: 0.9681 - val_acc: 0.9500\n",
      "Epoch 167/256\n",
      "80/80 [==============================] - 0s 938us/step - loss: 0.8786 - acc: 0.9250 - val_loss: 0.9589 - val_acc: 0.9500\n",
      "Epoch 168/256\n",
      "80/80 [==============================] - 0s 963us/step - loss: 0.8784 - acc: 0.9000 - val_loss: 0.9603 - val_acc: 0.9500\n",
      "Epoch 169/256\n",
      "80/80 [==============================] - 0s 950us/step - loss: 0.8506 - acc: 0.8875 - val_loss: 0.9472 - val_acc: 0.9500\n",
      "Epoch 170/256\n",
      "80/80 [==============================] - 0s 963us/step - loss: 0.8594 - acc: 0.9000 - val_loss: 0.9443 - val_acc: 0.9500\n",
      "Epoch 171/256\n",
      "80/80 [==============================] - 0s 963us/step - loss: 0.8565 - acc: 0.9125 - val_loss: 0.9457 - val_acc: 0.9500\n",
      "Epoch 172/256\n",
      "80/80 [==============================] - 0s 975us/step - loss: 0.8354 - acc: 0.9125 - val_loss: 0.9341 - val_acc: 0.9500\n",
      "Epoch 173/256\n",
      "80/80 [==============================] - 0s 963us/step - loss: 0.8433 - acc: 0.8625 - val_loss: 0.9325 - val_acc: 0.9500\n",
      "Epoch 174/256\n",
      "80/80 [==============================] - 0s 950us/step - loss: 0.8522 - acc: 0.9125 - val_loss: 0.9319 - val_acc: 0.9500\n",
      "Epoch 175/256\n",
      "80/80 [==============================] - 0s 950us/step - loss: 0.8896 - acc: 0.8750 - val_loss: 0.9244 - val_acc: 0.9500\n",
      "Epoch 176/256\n",
      "80/80 [==============================] - 0s 963us/step - loss: 0.8792 - acc: 0.8625 - val_loss: 0.9328 - val_acc: 0.9500\n",
      "Epoch 177/256\n",
      "80/80 [==============================] - 0s 950us/step - loss: 0.8111 - acc: 0.9125 - val_loss: 0.9215 - val_acc: 0.9500\n",
      "Epoch 178/256\n",
      "80/80 [==============================] - 0s 988us/step - loss: 0.8327 - acc: 0.9000 - val_loss: 0.9224 - val_acc: 0.9500\n",
      "Epoch 179/256\n",
      "80/80 [==============================] - 0s 988us/step - loss: 0.8333 - acc: 0.8875 - val_loss: 0.9154 - val_acc: 0.9500\n",
      "Epoch 180/256\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.8889 - acc: 0.8500 - val_loss: 0.9116 - val_acc: 0.9500\n",
      "Epoch 181/256\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.8277 - acc: 0.8875 - val_loss: 0.9089 - val_acc: 0.9500\n",
      "Epoch 182/256\n",
      "80/80 [==============================] - 0s 988us/step - loss: 0.8360 - acc: 0.8750 - val_loss: 0.9023 - val_acc: 0.9500\n",
      "Epoch 183/256\n",
      "80/80 [==============================] - 0s 975us/step - loss: 0.8570 - acc: 0.9125 - val_loss: 0.8979 - val_acc: 0.9500\n",
      "Epoch 184/256\n",
      "80/80 [==============================] - 0s 975us/step - loss: 0.8341 - acc: 0.8875 - val_loss: 0.8975 - val_acc: 0.9500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 185/256\n",
      "80/80 [==============================] - 0s 988us/step - loss: 0.8256 - acc: 0.9000 - val_loss: 0.8942 - val_acc: 0.9500\n",
      "Epoch 186/256\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.8017 - acc: 0.9125 - val_loss: 0.8929 - val_acc: 0.9500\n",
      "Epoch 187/256\n",
      "80/80 [==============================] - 0s 963us/step - loss: 0.8203 - acc: 0.9250 - val_loss: 0.8857 - val_acc: 0.9500\n",
      "Epoch 188/256\n",
      "80/80 [==============================] - 0s 950us/step - loss: 0.7920 - acc: 0.8750 - val_loss: 0.8867 - val_acc: 0.9500\n",
      "Epoch 189/256\n",
      "80/80 [==============================] - 0s 963us/step - loss: 0.8085 - acc: 0.8750 - val_loss: 0.8899 - val_acc: 0.9500\n",
      "Epoch 190/256\n",
      "80/80 [==============================] - 0s 950us/step - loss: 0.8443 - acc: 0.9000 - val_loss: 0.8781 - val_acc: 0.9500\n",
      "Epoch 191/256\n",
      "80/80 [==============================] - 0s 938us/step - loss: 0.8592 - acc: 0.8625 - val_loss: 0.8842 - val_acc: 0.9500\n",
      "Epoch 192/256\n",
      "80/80 [==============================] - 0s 950us/step - loss: 0.8185 - acc: 0.9000 - val_loss: 0.8809 - val_acc: 0.9500\n",
      "Epoch 193/256\n",
      "80/80 [==============================] - 0s 950us/step - loss: 0.7737 - acc: 0.8875 - val_loss: 0.8806 - val_acc: 0.9500\n",
      "Epoch 194/256\n",
      "80/80 [==============================] - 0s 975us/step - loss: 0.8178 - acc: 0.8625 - val_loss: 0.8686 - val_acc: 0.9500\n",
      "Epoch 195/256\n",
      "80/80 [==============================] - 0s 963us/step - loss: 0.7838 - acc: 0.9000 - val_loss: 0.8747 - val_acc: 0.9500\n",
      "Epoch 196/256\n",
      "80/80 [==============================] - 0s 950us/step - loss: 0.8207 - acc: 0.8625 - val_loss: 0.8628 - val_acc: 0.9500\n",
      "Epoch 197/256\n",
      "80/80 [==============================] - 0s 988us/step - loss: 0.8058 - acc: 0.9125 - val_loss: 0.8643 - val_acc: 0.9500\n",
      "Epoch 198/256\n",
      "80/80 [==============================] - 0s 988us/step - loss: 0.8040 - acc: 0.9125 - val_loss: 0.8585 - val_acc: 0.9500\n",
      "Epoch 199/256\n",
      "80/80 [==============================] - 0s 975us/step - loss: 0.8074 - acc: 0.9125 - val_loss: 0.8537 - val_acc: 0.9500\n",
      "Epoch 200/256\n",
      "80/80 [==============================] - 0s 938us/step - loss: 0.7944 - acc: 0.8875 - val_loss: 0.8541 - val_acc: 0.9500\n",
      "Epoch 201/256\n",
      "80/80 [==============================] - 0s 988us/step - loss: 0.7876 - acc: 0.9375 - val_loss: 0.8451 - val_acc: 0.9500\n",
      "Epoch 202/256\n",
      "80/80 [==============================] - 0s 938us/step - loss: 0.7875 - acc: 0.9125 - val_loss: 0.8462 - val_acc: 0.9500\n",
      "Epoch 203/256\n",
      "80/80 [==============================] - 0s 963us/step - loss: 0.7748 - acc: 0.9000 - val_loss: 0.8441 - val_acc: 0.9500\n",
      "Epoch 204/256\n",
      "80/80 [==============================] - 0s 963us/step - loss: 0.7774 - acc: 0.8500 - val_loss: 0.8393 - val_acc: 0.9500\n",
      "Epoch 205/256\n",
      "80/80 [==============================] - 0s 950us/step - loss: 0.7823 - acc: 0.9125 - val_loss: 0.8376 - val_acc: 0.9500\n",
      "Epoch 206/256\n",
      "80/80 [==============================] - 0s 975us/step - loss: 0.7630 - acc: 0.9375 - val_loss: 0.8417 - val_acc: 0.9500\n",
      "Epoch 207/256\n",
      "80/80 [==============================] - 0s 975us/step - loss: 0.7376 - acc: 0.9375 - val_loss: 0.8340 - val_acc: 0.9500\n",
      "Epoch 208/256\n",
      "80/80 [==============================] - 0s 963us/step - loss: 0.7638 - acc: 0.9375 - val_loss: 0.8285 - val_acc: 0.9500\n",
      "Epoch 209/256\n",
      "80/80 [==============================] - 0s 963us/step - loss: 0.7679 - acc: 0.8875 - val_loss: 0.8248 - val_acc: 0.9500\n",
      "Epoch 210/256\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.7585 - acc: 0.8875 - val_loss: 0.8228 - val_acc: 0.9500\n",
      "Epoch 211/256\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.7489 - acc: 0.9125 - val_loss: 0.8230 - val_acc: 0.9500\n",
      "Epoch 212/256\n",
      "80/80 [==============================] - 0s 988us/step - loss: 0.7927 - acc: 0.9000 - val_loss: 0.8195 - val_acc: 0.9500\n",
      "Epoch 213/256\n",
      "80/80 [==============================] - 0s 963us/step - loss: 0.7555 - acc: 0.8875 - val_loss: 0.8152 - val_acc: 0.9500\n",
      "Epoch 214/256\n",
      "80/80 [==============================] - 0s 975us/step - loss: 0.7469 - acc: 0.9250 - val_loss: 0.8108 - val_acc: 0.9500\n",
      "Epoch 215/256\n",
      "80/80 [==============================] - 0s 975us/step - loss: 0.7303 - acc: 0.9125 - val_loss: 0.8114 - val_acc: 0.9500\n",
      "Epoch 216/256\n",
      "80/80 [==============================] - 0s 950us/step - loss: 0.7515 - acc: 0.9250 - val_loss: 0.8051 - val_acc: 0.9500\n",
      "Epoch 217/256\n",
      "80/80 [==============================] - 0s 950us/step - loss: 0.7149 - acc: 0.9125 - val_loss: 0.8020 - val_acc: 0.9500\n",
      "Epoch 218/256\n",
      "80/80 [==============================] - 0s 988us/step - loss: 0.7491 - acc: 0.8750 - val_loss: 0.7964 - val_acc: 0.9500\n",
      "Epoch 219/256\n",
      "80/80 [==============================] - 0s 963us/step - loss: 0.7574 - acc: 0.9125 - val_loss: 0.7936 - val_acc: 0.9500\n",
      "Epoch 220/256\n",
      "80/80 [==============================] - 0s 963us/step - loss: 0.7174 - acc: 0.9250 - val_loss: 0.7924 - val_acc: 0.9500\n",
      "Epoch 221/256\n",
      "80/80 [==============================] - 0s 975us/step - loss: 0.7288 - acc: 0.9125 - val_loss: 0.7781 - val_acc: 0.9500\n",
      "Epoch 222/256\n",
      "80/80 [==============================] - 0s 975us/step - loss: 0.7511 - acc: 0.8875 - val_loss: 0.7877 - val_acc: 0.9500\n",
      "Epoch 223/256\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.7233 - acc: 0.9250 - val_loss: 0.7876 - val_acc: 0.9500\n",
      "Epoch 224/256\n",
      "80/80 [==============================] - 0s 988us/step - loss: 0.6858 - acc: 0.9500 - val_loss: 0.7749 - val_acc: 0.9500\n",
      "Epoch 225/256\n",
      "80/80 [==============================] - 0s 988us/step - loss: 0.7092 - acc: 0.9125 - val_loss: 0.7710 - val_acc: 0.9500\n",
      "Epoch 226/256\n",
      "80/80 [==============================] - 0s 988us/step - loss: 0.6934 - acc: 0.9250 - val_loss: 0.7733 - val_acc: 0.9500\n",
      "Epoch 227/256\n",
      "80/80 [==============================] - 0s 963us/step - loss: 0.7116 - acc: 0.9250 - val_loss: 0.7690 - val_acc: 0.9500\n",
      "Epoch 228/256\n",
      "80/80 [==============================] - 0s 988us/step - loss: 0.7338 - acc: 0.9125 - val_loss: 0.7687 - val_acc: 0.9500\n",
      "Epoch 229/256\n",
      "80/80 [==============================] - 0s 988us/step - loss: 0.7078 - acc: 0.8875 - val_loss: 0.7598 - val_acc: 0.9500\n",
      "Epoch 230/256\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.7097 - acc: 0.9125 - val_loss: 0.7624 - val_acc: 0.9500\n",
      "Epoch 231/256\n",
      "80/80 [==============================] - 0s 963us/step - loss: 0.7253 - acc: 0.9000 - val_loss: 0.7556 - val_acc: 0.9500\n",
      "Epoch 232/256\n",
      "80/80 [==============================] - 0s 963us/step - loss: 0.7342 - acc: 0.8500 - val_loss: 0.7552 - val_acc: 0.9500\n",
      "Epoch 233/256\n",
      "80/80 [==============================] - 0s 988us/step - loss: 0.7114 - acc: 0.9375 - val_loss: 0.7521 - val_acc: 0.9500\n",
      "Epoch 234/256\n",
      "80/80 [==============================] - 0s 988us/step - loss: 0.7399 - acc: 0.9000 - val_loss: 0.7541 - val_acc: 0.9500\n",
      "Epoch 235/256\n",
      "80/80 [==============================] - 0s 988us/step - loss: 0.6903 - acc: 0.9375 - val_loss: 0.7509 - val_acc: 0.9500\n",
      "Epoch 236/256\n",
      "80/80 [==============================] - 0s 988us/step - loss: 0.6716 - acc: 0.9375 - val_loss: 0.7462 - val_acc: 0.9500\n",
      "Epoch 237/256\n",
      "80/80 [==============================] - 0s 975us/step - loss: 0.6605 - acc: 0.8875 - val_loss: 0.7453 - val_acc: 0.9500\n",
      "Epoch 238/256\n",
      "80/80 [==============================] - 0s 988us/step - loss: 0.6541 - acc: 0.9125 - val_loss: 0.7392 - val_acc: 0.9500\n",
      "Epoch 239/256\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.7100 - acc: 0.8750 - val_loss: 0.7359 - val_acc: 0.9500\n",
      "Epoch 240/256\n",
      "80/80 [==============================] - 0s 988us/step - loss: 0.6460 - acc: 0.9375 - val_loss: 0.7348 - val_acc: 0.9500\n",
      "Epoch 241/256\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.6177 - acc: 0.9375 - val_loss: 0.7271 - val_acc: 0.9500\n",
      "Epoch 242/256\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.6739 - acc: 0.9250 - val_loss: 0.7249 - val_acc: 0.9500\n",
      "Epoch 243/256\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.6522 - acc: 0.9375 - val_loss: 0.7243 - val_acc: 0.9500\n",
      "Epoch 244/256\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.6460 - acc: 0.9125 - val_loss: 0.7207 - val_acc: 0.9500\n",
      "Epoch 245/256\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.6774 - acc: 0.9125 - val_loss: 0.7203 - val_acc: 0.9500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 246/256\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.6435 - acc: 0.9375 - val_loss: 0.7106 - val_acc: 0.9500\n",
      "Epoch 247/256\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.6544 - acc: 0.9125 - val_loss: 0.7112 - val_acc: 0.9500\n",
      "Epoch 248/256\n",
      "80/80 [==============================] - 0s 963us/step - loss: 0.6518 - acc: 0.9250 - val_loss: 0.7158 - val_acc: 0.9500\n",
      "Epoch 249/256\n",
      "80/80 [==============================] - 0s 950us/step - loss: 0.6385 - acc: 0.9375 - val_loss: 0.7122 - val_acc: 0.9500\n",
      "Epoch 250/256\n",
      "80/80 [==============================] - 0s 963us/step - loss: 0.6899 - acc: 0.9250 - val_loss: 0.7078 - val_acc: 0.9500\n",
      "Epoch 251/256\n",
      "80/80 [==============================] - 0s 950us/step - loss: 0.6552 - acc: 0.8875 - val_loss: 0.7020 - val_acc: 0.9500\n",
      "Epoch 252/256\n",
      "80/80 [==============================] - 0s 950us/step - loss: 0.6705 - acc: 0.9125 - val_loss: 0.7049 - val_acc: 0.9500\n",
      "Epoch 253/256\n",
      "80/80 [==============================] - 0s 963us/step - loss: 0.6568 - acc: 0.9125 - val_loss: 0.6996 - val_acc: 0.9500\n",
      "Epoch 254/256\n",
      "80/80 [==============================] - 0s 950us/step - loss: 0.6507 - acc: 0.9375 - val_loss: 0.7149 - val_acc: 0.9500\n",
      "Epoch 255/256\n",
      "80/80 [==============================] - 0s 963us/step - loss: 0.6412 - acc: 0.9375 - val_loss: 0.6928 - val_acc: 0.9500\n",
      "Epoch 256/256\n",
      "80/80 [==============================] - 0s 988us/step - loss: 0.6550 - acc: 0.9125 - val_loss: 0.6953 - val_acc: 0.9500\n"
     ]
    }
   ],
   "source": [
    "block_train_dropout = block_model.fit(train_X, train_label, batch_size=batch_size,epochs=epochs,verbose=1,validation_data=(valid_X, valid_label))\n",
    "\n",
    "# guardamos la red, para reutilizarla en el futuro, sin tener que volver a entrenar\n",
    "block_model.save(\"block_mnist.h5py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 0s 400us/step\n",
      "Test loss: 0.5648746490478516\n",
      "Test accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "test_eval = block_model.evaluate(test_X, test_Y_one_hot, verbose=1)\n",
    "\n",
    "print('Test loss:', test_eval[0])\n",
    "print('Test accuracy:', test_eval[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5648746490478516, 1.0]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "im = [plt.imread('trainprueba/Z/_1.png')]\n",
    "X = np.array(im, dtype=np.uint8)\n",
    "prediction = block_model.predict(X)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyautogui\n",
    "import time\n",
    "\n",
    "def screenshot(delay, name):\n",
    "    time.sleep(delay)\n",
    "    screenshot = pyautogui.screenshot()\n",
    "    screenshot.save(\"image/_\"+str(name)+\".png\")\n",
    "\n",
    "cont = 0\n",
    "while True:\n",
    "    screenshot(1, cont)\n",
    "    cont+=1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
